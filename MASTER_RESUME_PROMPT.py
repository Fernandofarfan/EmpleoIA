"""
ATS Resume Optimization Master Prompt - Production Version 3.0
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Integrated Improvements: #1, #2, #3, #5
Target ATS Score: 90+
Output Format: Plain Text (for LaTeX injection)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

USER_EXPERIENCE_CONTEXT = """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
LOYALTY JUGGERNAUT INC â€” Data Engineer (Jun 2021 â€“ Oct 2022)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

As part of the Data Lake & ETL Platform team, I designed, built, and maintained large-scale, production-grade data pipelines for multiple loyalty clients including HDFCâ€“Shoppers Stop, ICICI, Jumeirah, and FAH.

I architected a scalable ETL framework that automated member onboarding and enrichment across distributed systems â€” using a pipeline flow from SFTP â†’ S3 â†’ RDS (PostgreSQL) â†’ Redshift/DynamoDB.

I implemented multithreading with ThreadPoolExecutor, enabling concurrent data ingestion in chunks and reducing runtime from 4 hours to ~1.5 hours (â‰ˆ70% faster).

The system handled tier-based deduplication logic (mobile, DOB, tier hierarchy) using Spark window functions, achieving over 99% accuracy while preserving reward tier integrity. I used PySpark and AWS EMR for distributed transformations, schema enforcement, and data validation.

I later built Hudi-based incremental CDC sync between RDS, DynamoDB, and Redshift, ensuring consistency across transactional and analytical layers.

This framework was later adapted for other clients, becoming a reusable ingestion template used across the platform.

To modernize orchestration, I migrated cron-based jobs into Airflow DAGs, integrated API-based triggers for AWS Batch workloads, and introduced timeout, retry, and dynamic scaling configurations to handle long-running transformations.

On the DevOps side, I set up GitLab CI/CD pipelines and Jenkins jobs for deploying DAGs, managing schema migrations, and validating configurations.

I also integrated Terraform for infrastructure provisioning and configuration drift control.

For monitoring and reliability, I connected Datadog, Sentry, and CloudWatch, building custom alerting mechanisms (Slack + Email) to track ETL job failures and latency.

This reduced manual monitoring overhead by 60% and improved uptime to 99.5%.

I collaborated in Agile sprints via Jira, maintained documentation and design specs in Confluence, and mentored new developers in pipeline design and validation.

The overall data platform processed 10K+ daily records with 99.8% accuracy, 70% higher throughput, and 95% workflow automation.

**Stack:** Python, PySpark, Pandas, PostgreSQL, Redshift, DynamoDB, S3, Lambda, EMR, DMS, Airflow, AWS Batch, Terraform, GitLab CI/CD, Jenkins, Datadog, Sentry, CloudWatch, Jira, Confluence

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
DC WITNESS â€” Data Analyst Intern (Feb 2025 â€“ Apr 2025)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

At DC Witness, I developed an ETL automation pipeline to convert unstructured court case PDFs and emails into structured, analyzable data.

I used Python (Pandas, PyPDF2) for extraction, cleansing, and schema alignment, enabling the team to process 10K+ legal records efficiently.

I built Power BI dashboards to visualize timelines, verdict trends, and judge-level analytics, enabling reporters to identify case bottlenecks and outcomes.

Automated validation checks ensured 95% data accuracy and reduced manual verification by 70%.

I integrated the system with AWS S3 for storage and used SQL queries to join and transform datasets for visualization.

**Impact:** Eliminated manual PDF-to-Excel conversion and established a consistent data flow from ingestion to analytics delivery.

**Stack:** Python, Pandas, PyPDF2, SQL, Power BI, AWS S3

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
WOMEN OF CONNECTIONS â€” Data Integration & Automation Project (Jan 2025 â€“ Present)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

I built a serverless ETL pipeline that automated the synchronization of 500+ community resource entries (education, career, health) from Google Sheets into WordPress CMS using the WordPress REST API.

The pipeline, orchestrated via Apache Airflow, handled schema validation, data transformation, and incremental updates in real time.

I implemented error handling, logging, and alerts, achieving 95% data accuracy and reducing content update latency from 2 days to under 30 seconds.

This eliminated manual WordPress updates and established a scalable automation workflow for content publishing.

**Stack:** Python, Pandas, PostgreSQL, Google Apps Script, WordPress REST API, Airflow

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
AUTOMATED JOB DATA PIPELINE & ANALYTICS SYSTEM (Personal Project)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

I engineered an end-to-end data pipeline using Apache Airflow to automatically scrape job listings from Indeed and Google Jobs and store them in PostgreSQL.

The system included an NLP-based skill extraction module (using Python and Pandas) to calculate candidateâ€“JD match scores, improving targeting by 60%.

I built Power BI dashboards to track hiring trends, company-level demand, and skill frequency with daily updates.

The workflow was Dockerized for portability, included retry logic in Airflow DAGs for fault tolerance, and reduced manual job searching time by 80%.

**Stack:** Python, Airflow, Selenium, PostgreSQL, Pandas, NLP, Power BI, Docker

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
GEORGE MASON UNIVERSITY â€” Graduate Teaching & Research Assistant (Jan 2024 â€“ Dec 2024)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

As a Graduate Teaching Assistant, I supported courses in Data Structures and Database Systems, mentoring 30+ students in algorithmic efficiency and relational modeling.

As a Research Assistant, I contributed to developing an LLM-based time management assistant for computer science students.

I analyzed student behavior data, evaluated model performance using Python-based metrics, and proposed improvements to response structure and contextual reasoning.

**Stack:** Python, NLP, Prompt Engineering, Research Documentation

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
EDUCATION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Master's in Computer Science** â€” George Mason University (2023â€“2024)
- Relevant Coursework: Database Systems, Big Data Analytics, Machine Learning, Data Structures
- Graduate Teaching Assistant for Data Structures & Database Systems

**Bachelor's in Computer Science** â€” Sreenidhi Institute of Science & Technology (2015â€“2019)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
TECHNICAL SKILLS (Comprehensive & Interview-Ready)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Programming & Processing:** Python (Pandas, NumPy, PySpark), SQL, Shell scripting, ThreadPoolExecutor, Multithreading

**Data Engineering & Orchestration:** Apache Airflow, AWS Batch, Glue, EMR, DMS, Hudi, Terraform, Jenkins, GitLab CI/CD, REST APIs, dbt-style SQL modeling

**Cloud Platforms:** 
- AWS: S3, Lambda, Redshift, DynamoDB, CloudWatch, Glue, EMR, Batch, IAM
- Azure: Data Factory, Synapse Analytics, Blob Storage, Azure Functions, Azure Monitor
- GCP: BigQuery, Cloud Storage, Dataflow, Cloud Functions, Cloud Run

**Databases & Warehousing:** PostgreSQL, Redshift, DynamoDB, Snowflake, Hudi, MySQL, NoSQL

**Data Quality & Governance:** Schema validation, Deduplication, Data lineage, Reconciliation, Accuracy monitoring, Data profiling

**ETL/ELT Frameworks:** Batch & streaming pipelines, CDC (Change Data Capture), Incremental sync, Spark transformations, Schema evolution

**DevOps & Automation:** Docker, Terraform, Jenkins, GitLab CI/CD, Infrastructure-as-Code (IaC)

**Monitoring & Observability:** Datadog, Sentry, CloudWatch, Slack + Email alerts, Log aggregation

**Analytics & Visualization:** Tableau, Power BI, Excel, Google Data Studio â€” KPI dashboards, ROI analytics, trend monitoring

**Web & Data Automation:** Selenium, BeautifulSoup, Google Apps Script, WordPress REST API, NLP (Text-to-SQL, Skill Extraction)

**Collaboration & Documentation:** Jira, Confluence, Agile/Scrum, Technical documentation

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
CORE COMPETENCIES (Data Engineering Focused)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ“ Scalable pipeline design (ETL/ELT) for high-volume data ingestion
âœ“ Distributed processing with Apache Spark and parallel computing
âœ“ Data modeling (star schema, snowflake schema, dimensional modeling)
âœ“ Multi-cloud architecture (AWS/Azure/GCP platform expertise)
âœ“ Performance tuning (SQL optimization, partitioning, indexing, query planning)
âœ“ Data quality & governance (validation, lineage, profiling, reconciliation)
âœ“ Workflow orchestration (Airflow DAGs, scheduling, dependency management)
âœ“ Real-time streaming (Kafka, Kinesis, Event Hubs)
âœ“ CI/CD for data (Jenkins, dbt, automated testing, deployment automation)
âœ“ API integration (REST APIs, authentication, rate limiting, error handling)
âœ“ Infrastructure automation (Terraform, Docker, containerization)
âœ“ Production monitoring & alerting (Datadog, CloudWatch, SLA tracking)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
CLOUD PLATFORM SKILL SUBSTITUTION RULES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

If JD emphasizes Azure or GCP, substitute AWS equivalents truthfully:

AWS â†’ Azure â†’ GCP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
S3 â†’ Blob Storage â†’ Cloud Storage
Redshift â†’ Synapse Analytics â†’ BigQuery
Glue â†’ Data Factory â†’ Dataflow
Lambda â†’ Azure Functions â†’ Cloud Functions
Kinesis â†’ Event Hubs â†’ Pub/Sub
Batch â†’ Azure Batch â†’ Cloud Run
DMS â†’ Data Migration Service â†’ Datastream
EMR â†’ Databricks on Azure â†’ Dataproc
CloudWatch â†’ Azure Monitor â†’ Cloud Monitoring
IAM â†’ Azure AD/Entra ID â†’ Cloud IAM

**Example transformation:**
- Original: "Built ETL pipelines using AWS Glue and S3"
- If JD mentions Azure: "Built ETL pipelines using Azure Data Factory and Blob Storage"
- If JD mentions GCP: "Built ETL pipelines using Cloud Dataflow and Cloud Storage"

**Note:** User has ACTUAL hands-on experience with AWS services (S3, Glue, Redshift, Lambda, EMR, DMS, Airflow, Batch). Substitution is interview-safe because the underlying concepts, architecture patterns, and design principles are identical across cloud platforms. User can confidently discuss distributed systems, data partitioning, orchestration logic, and performance optimization regardless of platform.
"""

MASTER_RESUME_PROMPT = """
You are a professional resume optimization model specialized in Data Engineering, 
Data Platform, and Cloud Data roles. Your objective is to produce a resume that:
(1) Achieves a 90+ ATS score,
(2) Reads naturally to human recruiters,
(3) Matches the target job description's language and technology stack precisely,
(4) Remains 100% authentic to the user's actual experience.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
STRATEGIC GOAL:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ” Maximize keyword overlap (target 90+% match) with the target JD  
âœ” Replace AWS stack with Azure or GCP equivalents if required  
âœ” Add relevant JD-specific tools or skills that align with user's experience scope  
âœ” Maintain factual accuracy (no fabricated work or exaggerated claims)  
âœ” Ensure natural readability and conversational tone  

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ATS SUPER-OPTIMIZATION STRATEGY:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1ï¸âƒ£ **CLOUD PLATFORM ADAPTATION**
If the JD emphasizes:
- Azure â†’ Replace all AWS terms with Azure equivalents.
- GCP â†’ Replace all AWS terms with GCP equivalents.
- Multi-cloud â†’ Keep AWS but mention Azure/GCP alongside it.

Mapping guide:
AWS â†’ Azure â†’ GCP
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
S3 â†’ Blob Storage â†’ Cloud Storage  
Redshift â†’ Synapse Analytics â†’ BigQuery  
Glue â†’ Data Factory â†’ Dataflow  
Lambda â†’ Azure Functions â†’ Cloud Functions  
Kinesis â†’ Event Hubs â†’ Pub/Sub  
Batch â†’ Azure Batch â†’ Cloud Run  
DMS â†’ Data Migration Service â†’ Datastream  
EMR â†’ Databricks on Azure â†’ Dataproc  
CloudWatch â†’ Azure Monitor â†’ Cloud Monitoring  
IAM â†’ Azure AD â†’ Cloud IAM  

2ï¸âƒ£ **JD SKILL ENRICHMENT**
- Extract all tools, frameworks, and skills mentioned in JD.
- Cross-check which are missing from the base resume.
- Add them *organically* where relevant (e.g., if JD mentions Kafka, mention 
  "near real-time streaming using Kafka or Kinesis").
- Prioritize top 15 hard skills by frequency in JD.

âš ï¸ **SAFEGUARD (CRITICAL)**: Do not add any skill that was not logically possible 
within the user's scope (e.g., Kubernetes if the user only worked in serverless). 
Instead, reference adjacent or equivalent technologies truthfully.

ðŸ’¡ **STACK EXPANSION RULE**: If the JD includes a new but related stack 
(e.g., Databricks, Snowflake, dbt, Kafka), expand user's equivalent experience 
truthfully â€” e.g., 'PySpark on EMR' â†’ 'PySpark on Databricks', or 
'SQL transformations' â†’ 'dbt-style data modeling.'

3ï¸âƒ£ **KEYWORD DENSITY CONTROL**
- Maintain target 90+ keyword density match naturally.
- Spread critical JD keywords evenly across sections (summary, experience, skills).
- Avoid repetitive stuffing â€” rephrase naturally.

4ï¸âƒ£ **SKILL SYNTHESIS (AUTHENTIC INJECTION)**
When JD mentions a skill user hasn't explicitly stated but clearly aligns 
(e.g., "data governance" â†’ user did validation & lineage), infer it safely:

Example:
User did: "Built ETL validations and quality checks"
â†’ Add naturally: "Implemented data governance and lineage tracking frameworks"

5ï¸âƒ£ **ACTION VERB & IMPACT OPTIMIZATION**
Each bullet must:
- Start with an action verb (Architected, Engineered, Automated, Optimized)
- Contain at least one measurable result (% improvement, time reduction, accuracy gain)
- Reference one technical keyword from the JD

6ï¸âƒ£ **SELF-CHECK RULES**
Before final output:
- Ensure target 90+ JD keyword coverage
- Verify each bullet is factually defensible
- Ensure readability (smooth human rhythm, not keyword-dense noise)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
USER EXPERIENCE CONTEXT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{user_experience_context}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
TARGET JOB DESCRIPTION:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{jd_text}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
BASE RESUME TEXT (TO REWRITE & OPTIMIZE):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{base_resume_text}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
OUTPUT INSTRUCTIONS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. Rewrite the resume in plain text only. NO LaTeX, NO markdown, NO special formatting codes.
2. Structure sections as:
   - PROFESSIONAL SUMMARY
   - TECHNICAL SKILLS
   - EXPERIENCE
   - PROJECTS
   - EDUCATION
3. Reorder bullets to align top achievements with JD's highest priorities.
4. Include cloud-specific replacements and JD-skill enrichments.
5. Inject all added terms naturally (no corporate fluff).
6. Return ONLY plain text that can be copied and pasted directly.

âš ï¸ **TONE CONTROL**: Ensure the final text reads like a confident human 
professional â€” balanced, natural, and metric-focused. Avoid corporate 
clichÃ©s and redundancy.

âš ï¸ **FORMAT WARNING**: Do NOT use LaTeX commands, markdown formatting, or any special codes. 
Return clean, readable plain text that looks professional when printed or viewed in a text editor.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
OUTPUT FORMAT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

PROFESSIONAL SUMMARY:
[2-3 sentences directly mirroring JD focus areas and user strengths; 
include keywords like scalable, distributed, ETL, orchestration, Azure/AWS, 
data modeling, streaming, and analytics.]

TECHNICAL SKILLS:
[List in categories: Cloud, Data Processing, Orchestration, Databases, DevOps, 
Monitoring, Visualization; include both original stack and JD skills]

EXPERIENCE:
[Company] | [Role] | [Dates] | [Location]
â€¢ [Action verb + task + technology + metric + JD keyword]
â€¢ [Repeat for 4â€“6 bullets per job]
â€¢ [Ensure at least 60% of bullets mirror JD phrasing]

PROJECTS:
[Highlight key technical projects or automation pipelines with relevant JD stack alignment]

EDUCATION:
[List degree, university, and related assistantship or coursework]

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
TRANSFORMATION EXAMPLE (Before â†’ After):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**Before (AWS-focused):**
"Built ETL pipelines using AWS Glue and S3, improving processing time by 40%."

**After (Azure-focused JD):**
"Built ETL pipelines using Azure Data Factory and Blob Storage, improving 
processing time by 40% through optimized parallel processing and partitioning."

**After (GCP-focused JD):**
"Built ETL pipelines using Cloud Dataflow and Cloud Storage, improving 
processing time by 40% through optimized parallel processing and partitioning."

**After (Multi-cloud JD):**
"Built cloud-native ETL pipelines using AWS Glue, Azure Data Factory, and GCP 
Dataflow, improving processing time by 40% through optimized parallel processing."

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
FINAL QUALITY CHECK (MANDATORY):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… Resume includes target 90+ of relevant technical keywords from JD  
âœ… All AWS â†’ Azure/GCP substitutions applied accurately  
âœ… All JD tools/skills incorporated where contextually valid  
âœ… All bullets quantifiable and start with strong action verbs  
âœ… Resume reads naturally and is interview-safe  
âœ… No fabricated experience or irrelevant buzzwords  

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
BEGIN OUTPUT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

IMPORTANT: Return ONLY plain text. No LaTeX, no markdown, no formatting codes. 
Just clean, readable text that can be copied directly.
"""

def get_formatted_prompt(job_description: str, base_resume_text: str, 
                        user_context: str = USER_EXPERIENCE_CONTEXT) -> str:
    """
    Format the master prompt with actual JD, resume, and user context.
    
    Args:
        job_description: The target job description
        base_resume_text: The user's current resume text
        user_context: Detailed user experience context (defaults to USER_EXPERIENCE_CONTEXT)
    
    Returns:
        Fully formatted prompt ready for Gemini
    """
    return MASTER_RESUME_PROMPT.format(
        user_experience_context=user_context,
        jd_text=job_description,
        base_resume_text=base_resume_text
    )
